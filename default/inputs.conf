##################################
#			nmon2csv stanza			#
##################################

# The TA-nmon_selfmode does not use the archive processor and either Splunk to manage nmon files
# Instead of this, it uses a scheduled script to identify nmon files to be managed

[script://./bin/nmon_manage.sh]
disabled = false
index = nmon
interval = 60
source = nmon_processing
sourcetype = nmon_processing

####################################################
#			nmon csv converted files indexing			#
####################################################

# Every file present within this directory will be indexed then deleted
# This section should not be modified under normal use

[batch://$SPLUNK_HOME/var/log/nmon/var/csv_repository/*nmon*.csv]

disabled = false
move_policy = sinkhole
recursive = false
crcSalt = <SOURCE>
index = nmon
sourcetype = nmon_data
# source override: to prevent Metadata from having millions of entry, the source is overriden by default
# You can disable this for trouble shooting purposes if required
source = perfdata

####################################################
#			nmon config files indexing						#
####################################################

# Every file present within this directory will be indexed then deleted
# This section should not be modified under normal use

[batch://$SPLUNK_HOME/var/log/nmon/var/config_repository/*nmon*.csv]

disabled = false
move_policy = sinkhole
recursive = false
crcSalt = <SOURCE>
index = nmon
sourcetype = nmon_config
# source override: to prevent Metadata from having millions of entry, the source is overriden by default
# You can disable this for trouble shooting purposes if required
source = configdata

####################################################
#					nmon data collect							#
####################################################

# These input script sanza will run nmon and generates nmon file 
# to be exploited by Splunk

# For AIX / Linux / Solaris

# NEW: Starting with v1.5.0, the nmon_helper.sh main options can be managed by the nmon.conf file, and 
# the script won't kill anymore any running nmon but check for it and exit without no action if a PID is found
# In default configuration, check every 60 seconds that nmon is running

[script://./bin/nmon_helper.sh]
disabled = false
index = nmon
interval = 60
source = nmon_collect
sourcetype = nmon_collect

####################################################
#		nmon cleaner
####################################################

# NEW with v1.5.0: A cleaner script is provided in Python and Perl to manage the cleaning of
# Nmon raw data files and csv perf and config files
# In normal circumstances, there should never be any csv files in csv_respository and config_repository, this action is off by default
# and unless you ask it, cleaner scripts won't do anything in these directories
# But if you want to prevent from running out of file handle or abnormal CPU usage in case of unexpected situation (eg. batch mode not working for example)
# you can use the "--cleancsv" switch (see the full help above)

# By default and without any option provided, cleaner scripts (nmon_cleaner.py and nmon_cleaner.pl) will:
# - Search for nmon files (*.nmon) in default nmon_repository and remove any file older than 24 hours

# Available options are:
# --cleancsv :Activate the purge of csv files from csv repository and config repository (see also options above)
# --maxseconds_csv <value> :Set the maximum file retention in seconds for csv data, every files older than this value will be permanently removed
# --maxseconds_nmon <value> :Set the maximum file retention in seconds for nmon files, every files older than this value will be permanently removed
# --approot <value> :Set a custom value for the Application root directory (default are: nmon / TA-nmon / PA-nmon)
# --csv_repository <value> :Set a custom location for directory containing csv data (default: csv_repository)
# --config_repository <value> :Set a custom location for directory containing config data (default: config_repository)
# --nmon_repository <value> :Set a custom location for directory containing nmon raw data (default: nmon_repository)
# --version :Show current program version

# NOTE: When manageing larget set of cold Nmon data generated out of Splunk, it is recommended to set an higher custom value for csv max retention
# The default value is 10 minutes, in case of high load of Splunk, this can lead to delete data before it is being indexed.
# When Splunk generates the Nmon data, we assume csv volatile data should have indexed in 10 minutes max.

# NEW since Version 1.5.07, a frontal simple shell script is used to encapsulate both Python and Perl script
# If Python is not locally available, the Perl version will be automatically used

[script://./bin/nmon_cleaner.sh --cleancsv]
disabled = false
index = nmon
interval = 600
source = nmon_cleaner
sourcetype = nmon_clean

###########################################
#			SA-Eventgen								#
###########################################

# All these stanza are related to Eventgen configuration, for App testing purposes, Evengen will generate various sample data
# These settings can be safety ignored in Production systems as they will never affect anything as long as Eventgen inputs are not activated

[batch://$SPLUNK_HOME/var/log/nmon/eventgen/*sample.csv]

disabled = true
move_policy = sinkhole
recursive = false
crcSalt = <SOURCE>
index = nmon
sourcetype = nmon_data